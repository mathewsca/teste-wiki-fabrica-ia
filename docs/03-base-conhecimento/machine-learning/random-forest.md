# üå≥ Random Forest

> **Status:** üü® Em revis√£o     
> **Tags:** #Supervisionado #Classificacao #Regressao #Ensemble


---


## üìù O que √© o modelo?

O **Random Forest** √© um algoritmo de aprendizado **supervisionado** que utiliza a t√©cnica de **Ensemble Learning**, ou aprendizado por conjunto. Em vez de confiar em apenas uma [√Årvore de Decis√£o](arvore-decisao.md), o modelo cria uma "floresta", com v√°rias √°rvores independentes, e assim combina os resultados individuais para obter uma previs√£o mais robusta e precisa, que seria melhor que apenas uma √∫nica √°rvore.

![Random Tree](https://www.researchgate.net/publication/354354484/figure/fig4/AS:1080214163595269@1634554534720/Illustration-of-random-forest-trees.jpg)


## üí° Como funciona?

> Para avaliar um filme, √© necess√°rio coletar a opini√£o de diferentes cr√≠ticos de cinema. Cada cr√≠tico tem uma percep√ß√£o diferente sobre o filme, por ser profissional como cineasta, diretor, ator, ou ser um cin√©filo e grande f√£ de cinema por ter assistindo muitos filmes! Desse modo, com uma regress√£o sobre a nota m√©dia, ou a classifica√ß√£o sobre opini√£o da maioria, se o filme √© bom, m√©dio ou ruim. A decis√£o do grupo tende a ser muito mais assertiva que a de um indiv√≠duo isolado, isso √© conhecido como sabedoria da multid√£o!


### L√≥gica do modelo
O modelo consiste em base na aleatoriedade, obtida do seguinte modo:

1. **Bagging**: √â a abrevia√ß√£o de *bootstrap aggregating*, parte do conjunto principal dos metodos de *Ensemble Learning*, que consiste em cada √°rvore da floresta √© treinada com uma amostragem aleat√≥ria dos dados originais com reposi√ß√£o. Para garantir que uma √°rvore tenha acesso aos mesmos dados que as outras;

2. **Feature Randomness:** Ao decidir como dividir um n√≥, o algoritmo escolhe um subconjunto aleat√≥rio de colunas, as *features*. Para evitar que uma vari√°vel dominante crie um padr√£o de √°rvores semelhantes;

3. **Agrega√ß√£o:** Para o m√©todo de *classifica√ß√£o*, ocorre o processo de vota√ß√£o majorit√°ria, ou seja, a classe mais votada vence. No m√©todo de *regress√£o*, √© calculado a m√©dia aritm√©tica dos resultados de todas as √°rvores;

### Classifica√ß√£o

Como mencionado no in√≠cio, os modelos de *random tree* podem ser utilizados tanto para **classifica√ß√£o** como tamb√©m para **regress√£o**. Nos modelos de classifica√£o a inten√ß√£o √© agrupar dados em classes diferentes ou conjuntos predefinidos.

### Regress√£o

Nos modelos de *random tree* para regress√£o, o objetivo √© prever valores cont√≠nuos, ou seja, pre√ßos, temperatura, quantidade, etc., que podem variar dado um intervalo. 


## ‚öô Principais Hiperpar√¢metros

Os hiperpar√¢metros s√£o configura√ß√µes de um modelo que podem alterar seu desempenho e efic√°cia. Diferente dos par√¢metros do modelo, que s√£o ajustados automaticamente durante o treinamento, os hiperpar√¢metros devem ser definidos antes do processo de aprendizado.

Hiperpar√¢metros mal ajustados podem levar a problemas como overfitting, onde o modelo se ajusta excessivamente aos dados de treinamento, ou underfitting, onde o modelo n√£o consegue capturar a complexidade dos dados.

Como o *Random Forest* √© um conjunto de √°rvores, s√£o hiperpar√¢metros semelhantes ao da √Årvore de Decis√£o, adicionando controles espec√≠ficos sobre a floresta inteira, com '*' ao lado dos hiperpar√¢metros de √°rvore de decis√£o:

| Hiperpar√¢metro | Fun√ß√£o | Impacto |
| :--- | :--- | :--- |
| `n_stimators` | N√∫mero de √°rvores utilizadas na floresta. | Melhora a estabilidade, mas aumenta o tempo de processamento.
| `max_features` | Quantidade de colunas que ser√£o sorteadas para cada divis√£o. | Aumenta a diversidade entre as √°rvores, reduzindo o overfitting.
| `bootstrap` | Se deve ou n√£o usar amostras aleat√≥rias com reposi√ß√£o. | `False`, cada √°rvore utilizar√° o dataset inteiro, n√£o recomendado. |
| *`max_depth` | Profundidade m√°xima da √°rvore. | Mais complexidade, aumentando o risco de overfitting. |
| *`min_samples_split` | M√≠nimo de amostras para criar uma nova divis√£o. | Divis√µes mais espec√≠ficas, maior risco de overfitting. |
| *`min_samples_leaf` | M√≠nimo de amostras que uma "folha" deve ter | Suaviza o modelo, evita folhas com apenas um dado. |
| *`criterion` | Fun√ß√£o que mede a qualidade da divis√£o. | Op√ß√£o de `gini`, para ser mais r√°pido, ou `entropy`, para ser mais rigoroso |
| *`random_state` | Gerador de n√∫mero rand√¥micos | Geralmente utilizado para garantir a replicabilidade dos experim√™ntos.
| `oob_score` | Utiliza dados que ficaram de fora do treino para validar o modelo. | Uma forma mais simples para testar a acur√°cia, sem a necessidade de configurar um set espec√≠fico de valida√ß√£o.

## ‚ö†Ô∏è Aten√ß√£o aos erros de principiante!
* **Quanto mais √°rvores, melhor!:** Ap√≥s certo n√≠vel, o ganho de acur√°cia √© reduzido, mas o custo computacional e o tamanho do arquivo do modelo continuam aumentando;

* **Perder interpretabilidade:** Quando se utiliza apenas uma √°rvore √© simples entender a l√≥gica dela, mas quando se utilizam 500 ou 1000 √°rvores √© mais complicado desenhar a l√≥gica de todas elas. Desse modo, √© necess√°rio utilizar outros m√©todos, como o `feature_importances_`;

* **N√£o tratar Outliers em Regress√£o:** Embora seja modelo robusto, o *Random Forest* n√£o consegue prever valores fora do intervalo encontrado no treino, ele n√£o extrapola tend√™ncias como uma regress√£o linear;

## üìà Vantagens & Limita√ß√µes

| ‚úÖ Vantagens | ‚ùå Limita√ß√µes |
|:--- | :--- |
| **Alta precis√£o:** Geralmente performa melhor que uma √°rvores simples e outros modelos lineares, dependendo do problema. | **Caixa-Preta:** Dif√≠cil de explicar visualmente o "porqu√™" de cada decis√£o individual, sendo necess√°rio outros m√©todos para interpretabilidade. |
| **Robustez overfitting:** A m√©dia de v√°rias √°rvores lida melhor com os erros de memoriza√ß√£o individuais. | **Lentid√£o em Tempo Real:** Caso a floresta seja muito grande, ou seja, existam muitas √°rvores, a predi√ß√£o pode ficar lenta para aplica√ß√µes onde tempo de processamento e lat√™ncia sejam prioridade.|
| **Lida com dados faltantes:** Consegue manter boa performance mesmo com valores faltantes. | **Consumo de Mem√≥ria:** O modelo salvo em disco pode ocupar espa√ßo razo√°vel dependendo do n√∫mero de √°rvores. |


## üß© Quando usar?
* **Dados Complexos:** Quando a rela√ß√£o entre as vari√°veis n√£o √© linear e existem muitas intera√ß√µes entre elas;

* **Ranking de Import√¢ncia:** Para descobrir quais vari√°veis s√£o realmente cruciais para o seu problema de neg√≥cio, utilizando a *Feature Selection*;

### üìã Aplica√ß√µes

* **:** O algoritmo pode analisar transa√ß√µes, valores, localiza√ß√£o, hist√≥rico do cliente, etc. e classificar as transa√ß√µes com "fradulentas" ou "n√£o fraudulentas";

* **Detec√ß√£o de Fraude:** O algoritmo pode analisar transa√ß√µes, valores, localiza√ß√£o, hist√≥rico do cliente, etc. e classificar as transa√ß√µes com "fradulentas" ou "n√£o fraudulentas";


## üéÆ Implementa√ß√£o R√°pida (Python)
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Inicializando a floresta com 100 √°rvores
model = RandomForestClassifier(
    n_estimators=100, 
    max_depth=10, 
    random_state=42
)

# Treino e Predi√ß√£o
model.fit(X_train, y_train)
previsoes = model.predict(X_test)

# Analisando a import√¢ncia das colunas
importances = model.feature_importances_
```


## üîó Refer√™ncias e Links Adicionais
- [Documenta√ß√£o Oficial Scikit-Learn (Ensembles: Gradient boosting, random forests, bagging, voting, stacking)](https://scikit-learn.org/stable/modules/ensemble.html)