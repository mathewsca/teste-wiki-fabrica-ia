# üöÄ XGBoost (eXtreme Gradient Boosting)
> Status: üü® Em revis√£o     
> Tags: #Supervisionado #Classificacao #Regressao #Ensemble #Boosting

---

## üìù O que √© o modelo?

O **XGBoost** √© uma implementa√ß√£o otimizada do algoritmo de **Gradient Boosting**, utilizado para **classifica√ß√£o** e **regress√£o**. Ele pertence √† categoria de **Ensemble Learning**, mas ao contr√°rio do [Random Forest](random-forest.md), onde as √°rvores s√£o independentes, no XGBoost as √°rvores s√£o constru√≠das sequencialmente, utilizando uma abordagem aditiva. Cada nova √°rvore tenta corrigir os erros cometidos pelas √°rvores anteriores.

![XGBoost](https://assets.ibm.com/is/image/ibm/6-1_sequential-ensemble-learning_boosting:16x9?dpr=on%2C1&wid=1536&hei=864)
## üí° Como funciona?

> Um professor passa um desafio de matem√°tica de 100 quest√µes para uma turma:  
    > O Primeiro aluno resolve a prova, acerta 60 quest√µes e erra 40.  
    > O Segundo aluno n√£o tenta resolver a prova inteira do zero. O professor entrega para ele apenas as 40 quest√µes que o primeiro errou. Dessas 40, erra 10.  
    > O Terceiro aluno √© chamado para focar exclusivamente nesses 10 erros restantes. Assim por diante...  

O funcionamento do **XGBoost** segue esse mesmo princ√≠pio, um aprendizado sequencial focado na corre√ß√£o de erros ou res√≠duos do anterior. O modelo aprende errando, corrigindo e se especializando progressivamente.

### L√≥gica do modelo

* **Gradiente:** Utiliza o c√°lculo de derivadas da fun√ß√£o de perda para saber em qual dire√ß√£o deve ajustar os pesos para diminuir o erro;

* **Regulariza√ß√£o:** O modelo possui penalidades embutidas, L1 e L2, o que o torna mais resistente ao overfitting do que o Gradient Boosting comum;

* **Sparsity Aware:** Com um algoritmo, aprende automaticamente como lidar com dados faltantes, decidindo qual o melhor caminho em cada n√≥;

![Boosting](https://www.dailydoseofds.com/content/images/size/w1000/2024/08/image-61.png)

## ‚öô Principais Hiperpar√¢metros

Os hiperpar√¢metros s√£o configura√ß√µes de um modelo que podem alterar seu desempenho e efic√°cia. Diferente dos par√¢metros do modelo, que s√£o ajustados automaticamente durante o treinamento, os hiperpar√¢metros devem ser definidos antes do processo de aprendizado.

Hiperpar√¢metros mal ajustados podem levar a problemas como overfitting, onde o modelo se ajusta excessivamente aos dados de treinamento, ou underfitting, onde o modelo n√£o consegue capturar a complexidade dos dados.

Estes s√£o os hiperpar√¢metros para controle de √°rvores de gradient boosted:

| Hiperpar√¢metro | Fun√ß√£o | Impacto |
| :--- | :--- | :--- |
``learning_rate`` | Conhecido como "eta", define o "tamanho do passo" de corre√ß√£o de cada √°rvore. | Variando entre 0 e 1. Menores valores exigem mais √°rvores, mas tornam o modelo mais robusto. |
``n_estimators`` | N√∫mero de √°rvores. | Mais √°rvores aumentam a capacidade, mas tamb√©m a complexidade do modelo, levando tamb√©m ao aumento do tempo de treinamento quanto a capacidade do modelo. √Årvores em excesso podem levar ao overfitting |
``max_depth`` | Profundidade individual de cada √°rvore. | Captura rela√ß√µes mais complexas, mas √© o principal causador de overfitting. Por conven√ß√£o o valor *default* √© 6 |
``gamma`` | Conhecido como *multiplicador de Lagrange*, controla a quantidade m√≠nima de redu√ß√£o de perda necess√°ria para realizar uma nova divis√£o em um n√≥ folha das √°rvores. | Quanto menor o valor, evita divis√µes desnecess√°rias e mais r√°pido √© o treinamento, por√©m podendo n√£o encontrar uma solu√ß√£o suficientemente adequada.
``subsample``| Porcentagem dos dados usados para treinar cada √°rvore. |	Adiciona aleatoriedade e ajuda a evitar o "v√≠cio" nos dados de treino. |

## ‚ö†Ô∏è Aten√ß√£o aos erros de principiante!
* **Early stopping:** Recomenda-se observar o erro durante o treinamento, visto que, treinar 1000 √°rvores se o erro parou de cair na √°rvore 200 √© desperd√≠cio de recurso computacional e tempo. Para isso, o  ``early_stopping_rounds`` permite a parada do treinamento do modelo, caso n√£o tenha evolu√ß√£o significativa durante um n√∫mero espec√≠fico de rodadas;

* **Learning rate muito alta:** Recomenda-se utilizar valores entre 0.01 at√© 0.1, j√° que valores como 0.3 ou 0.5 avan√ßam muito r√°pido e tornam o treinamento instavel. Enquanto valores muito baixos, podem levar muito tempo para convergir. Para compensar, pode-se aumentar ``n_estimators`` para manter o desempenho do modelo sem sacrificar a estabilidade do treinamento;

* **Ignorar a escala das labels em regress√£o:** O XGBoost √© robusto em lidar problemas de regress√£o com valores muito discrepantes. No entanto, recomenda-se aplicar escalonamento ou padroniza√ß√£o, afim de evitar vi√©ses na import√¢ncia das features. Para isso, frameworks como ``StandardScaler`` e ``MinMaxScaler`` podem ter impacto positivo no desempenho do XGBoost ao lidar com valores de entrada grandes;

## üìà Vantagens & Limita√ß√µes

| ‚úÖ Vantagens | ‚ùå Limita√ß√µes |
|:--- | :--- |
**Velocidade:** Processamento paralelo e otimiza√ß√£o de cache de CPU. | **Black box:** Dif√≠cil para explicar visualmente o caminho e a l√≥gica de diversas √°rvores sequenciais.  |
**Robustez para valores faltantes:** Com um algoritmo que reconhece a esparsidade dos dados, mantendo um bom desempenho se houver valores nulos no dataset. | **Mem√≥ria:** Pode consumir muita RAM se o dataset for massivo e n√£o houver otimiza√ß√£o de tipos ou t√©cnicas para adaptar o desempenho. |
**Regulariza√ß√£o nativa:** Apresenta uma menor tend√™ncia a "decorar" o ru√≠do. | **Curva de aprendizado mais alta:** Devido a alta personaliza√ß√£o, pode ser mais dif√≠cil de ajustar e configurar corretamente em compara√ß√£o com outros algoritmos. |


## üß© Quando usar?
* **Performance m√°xima e competi√ß√µes:** Quando as prioridades s√£o as m√©tricas de efici√™ncia e acur√°cia, como em competi√ß√µes no [Kaggle](https://www.kaggle.com/). No entando, ser√° importante considerar a disponibilidade de recursos computacionais e se a velocidade do tempo de treinamento √© justific√°vel dado o poder preditivo;

### üìã Aplica√ß√µes
* **Previs√£o de vendas:** Utilizado para modelagem preditiva, tendo em vista o bom desempenho para capturar sazonalidades e tend√™ncias complexas em dados hist√≥ricos de vendas;

* **Risco de cr√©dito:** Identificar padr√µes em comportamentos de cr√©dito de clientes e prever o risco de opera√ß√µes banc√°rias;


## üéÆ Implementa√ß√£o R√°pida (Python)
``` python
import xgboost as xgb
from sklearn.model_selection import train_test_split

# 'binary:logistic' para classifica√ß√£o
# 'reg:squarederror' para regress√£o

model = xgb.XGBClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=6
)

# Treino com early stopping
model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    verbose=False
)

# Predi√ß√£o
previsoes = model.predict(X_test)
```

## üîó Refer√™ncias e Links Adicionais
* [Documenta√ß√£o Oficial do XGBoost](https://xgboost.readthedocs.io/en/stable/#)
* [Documenta√ß√£o do Scikit Learn para XGBoost de Classifica√ß√£o (GradientBoostingClassifier)](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)
* [Documenta√ß√£o do Scikit Learn para XGBoost de Regress√£o (GradientBoostingRegressor)](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)
* [F√≥rum de XGBoost](https://xgboosting.com/)